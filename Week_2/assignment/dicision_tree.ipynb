{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree exercise\n",
    "You should use decision tree to classify. \n",
    "\n",
    "Design your DecisionTree. Do binary classification or multiclass classification (selected by yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle in-class Competetion\n",
    "请先前往 Kaggle 下载本次比赛的数据集\n",
    "\n",
    "比赛页面：https://inclass.kaggle.com/c/hdu-cama/leaderboard\n",
    "\n",
    "本次比赛可使用的 Package: Pandas, Numpy 以及系统内置库如 math 等\n",
    "\n",
    "完成下面代码后，使用 predict 函数对 test.csv 中的数据做出预测并将结果保存至一个 .csv 文件，然后 submit 至 Kaggle，可参考示例文件 sample.csv\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__\n",
    "\n",
    "__请务必仔细阅读 Kaggle 页面的各项信息__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "## 请务必仔细阅读文件 “ID3 Algorithm for Decision Trees.pdf”\n",
    "### Calculate Shannon Entropy\n",
    "\n",
    "熵是对不确定性的测量，熵越高，代表信息量越高，这里你需要使用熵来选择作为节点的特征。（选择能够最小化两边熵的特征）\n",
    "\n",
    "$$Entropy(S) = - P_+ \\log_2{P_+} - P_- \\log_2{P_-}$$\n",
    "\n",
    "### Calculate Information Gain\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_{v\\in Values(A)}{\\frac{|S_v|}{|S|}Entropy(S_v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data from train.csv and y_train.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateShannonEntropy(features, labels):\n",
    "    # Todo 1: calculate the entropy given a dataset\n",
    "    \n",
    "    total_entropy = None\n",
    "    return total_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_best_feature_to_split(features, labels):\n",
    "    # Todo 2: return the best feature based on the maximum number of information gain\n",
    "    \n",
    "    best_feature = 0\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(features, labels, current_feature, value):\n",
    "    # Todo 3: Split the dataset via current selected feature and it's value\n",
    "    # For example, when current_feature is TLS(top-left-square), and the value is 'o', \n",
    "    # the task is that return the subdataset in which all \"TLS\" is equal to 'o'\n",
    "    \n",
    "    sub_dataset = None\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_decision_tree(features, labels):\n",
    "    # Todo 4: Create a decision tree by recursion\n",
    "    #\n",
    "    # Tips: Set appropriate boundary conditions; \n",
    "    #       think about the values one by one; \n",
    "    #       Use the three functions defined before.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get test_dataset from test.csv\n",
    "\n",
    "def predict(decision_tree, test_dataset):\n",
    "    # Todo 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化\n",
    "在上面这些步骤完成后，你可以优化 create_decision_tree 函数以防止过拟合\n",
    "\n",
    "- 对决策树进行剪枝\n",
    "- 也推荐两个更简单又十分有效的办法\n",
    "    - 设置树的最大深度 max_depth\n",
    "    - 设置每个叶节点的最小 samples 数\n",
    "    - 这里可以参考 [decision tree in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) 中的参数设置以及其原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
